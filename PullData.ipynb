{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Git Repository Analysis Data Extraction\n",
    "\n",
    "This notebook helps you load historical git and file statistics data from any local git repository"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "The code assumes you have the following libraries installed:\n",
    "\n",
    "- pydriller\n",
    "- pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Output file paths and repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This repository path should be a LOCAL path already cloned on disk\n",
    "repository_path = 'D:\\\\OneDrive\\\\Documents\\\\FU\\\\DATA605\\\\FinalProject'\n",
    "\n",
    "# Declare our paths of interest\n",
    "commit_data_path = 'Commits.csv'\n",
    "file_commit_data_path = 'FileCommits.csv'\n",
    "file_size_data_path = 'FileSizes.csv'\n",
    "author_data_path = 'Authors.csv'\n",
    "merged_data_path = 'MergedFileData.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Pulling Commit Data from GitHub\n",
    "\n",
    "We will use the PyDriller library to pull commit data from GitHub for a public repository or local git repository and build a list of commits.\n",
    "\n",
    "This process can take a very long time depending on the size of the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Git Repository at D:\\OneDrive\\Documents\\FU\\DATA605\\FinalProject\n",
      "Fetching commits. This will take a long time...\n",
      "Saving Commits\n",
      "Saved to Commits.csv\n",
      "Fetching file commits. This will take a very long time...\n",
      "Saving Commits\n",
      "Saved to FileCommits.csv\n",
      "Repository Data Pulled Successfully\n"
     ]
    }
   ],
   "source": [
    "from Scripts import GitAnalyzer \n",
    "\n",
    "# This will pull all repository data and write to Commits.csv and FileCommits.csv\n",
    "GitAnalyzer.analyze_repository(repository_path, commit_data_path, file_commit_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building File Size Information\n",
    "\n",
    "Next we'll walk the directory looking at source files and build out a CSV file with all file data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file metrics from D:\\OneDrive\\Documents\\FU\\DATA605\\FinalProject\n",
      "4 source files read from D:\\OneDrive\\Documents\\FU\\DATA605\\FinalProject\n",
      "File size information saved to FileSizes.csv\n"
     ]
    }
   ],
   "source": [
    "from Scripts import FileAnalyzer\n",
    "\n",
    "FileAnalyzer.build_file_sizes(repository_path, file_size_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a List of Authors\n",
    "\n",
    "Next, we'll determine the unique authors for the code and save that to a separate file called Authors.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading commit data from Commits.csv\n",
      "Saved author information to Authors.csv\n"
     ]
    }
   ],
   "source": [
    "from Scripts import GitAuthors\n",
    "\n",
    "GitAuthors.identify_authors(commit_data_path, author_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Merging Data Together\n",
    "\n",
    "Next, we're going to unify all the data together into a single MergedFileData.csv file that lets us do more in-depth analysis against a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts import GitDataMerger\n",
    "\n",
    "GitDataMerger.generate_merged_data(file_commit_data_path, file_size_data_path, merged_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4868653bb6f8972e87e4c446ab8a445a15b25dedb8594cc74c480f8152ea86a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
